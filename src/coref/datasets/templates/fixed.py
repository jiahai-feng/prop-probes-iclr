'''
Contains templates that are constrained to have a fixed number of entities.
Templates are typically generated by GPT.
'''
import os
import yaml
import torch
from coref.datasets.domains.simple import (
    CapitalDomain,
    NameDomain,
    HobbyDomain,
    ObjectDomain,
    AlphabetDomain,
    FoodDomain,
    OccupationDomain,
    NounPhraseDomain,
    VerbPhraseDomain
)
from coref import COREF_ROOT
from coref.datasets.domains.common import set_prompt_id
import coref.datasets.templates.common as tc
from coref.datasets.templates.common import TrackFormatter, Substring, chat_formatter


class NameCountryFoodFixedTemplate:
    class statement_formatters:
        def parallel(self, names, countries, foods):
            return dict(
                [(f"name_{i}", self.names[name]) for i, name in enumerate(names)]
                + [
                    (f"country_{i}", self.capitals[attr][0])
                    for i, attr in enumerate(countries)
                ]+ [
                    (f"food_{i}", self.foods[attr])
                    for i, attr in enumerate(foods)
                ]
            )
    default_statement_type = "parallel"
    prompt_templates = {
        "chat_name_country": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} lives in the country of"]),
        "chat_name_food": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} likes to eat"]),
    }
    basic_context_template = (
        "{name_0} lives in {country_0}. {name_1} lives in {country_1}. "
        "{name_0} likes to eat {food_0}. {name_1} likes to eat {food_1}."
    )

    def __init__(self, tokenizer_type, split=None):
        self.default_template_content = dict(
            prompt_type = "chat_name_country",
            chat_style="llama_chat",
            context_type = 0, # "basic", "random", or int
            prompt_prefix = "",
            context_prefix = "",
            chat_history = [],
        )
        self.names = NameDomain(tokenizer_type=tokenizer_type, split=split)
        self.capitals = CapitalDomain(tokenizer_type=tokenizer_type, split=split)
        self.foods = FoodDomain(tokenizer_type=tokenizer_type, split=split)
        self.tokenizer = self.names.tokenizer

        with open(os.path.join(COREF_ROOT, "coref/datasets/auto_gen/gen_templates/name_country_food_2.yaml")) as f:
            self.context_templates = yaml.safe_load(f)

    def extract_template_indices(self, full_output_indices):
        return {
            "qn_subject": full_output_indices["qn_subject"][0],
            "ans_subject": full_output_indices["qn_subject"][-1],
        }

    def generate_template(
        self, prompt_id, template_content, context_content, num_answers
    ):
        """
        Returns:
            (prompt : str, template_substitutions : Dict, answer: List[int])
        """
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_content.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            with set_prompt_id(prompt_id, self.names, self.capitals, self.foods):
                if prompt_type == "chat_name_country":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.capitals.encode_single_word(self.capitals[i][0])
                        for i in range(num_answers)
                    ]
                elif prompt_type == "chat_name_food":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.foods.encode_single_word(self.foods[i], suppress_error=True)
                        for i in range(num_answers)
                    ]
                else:
                    raise ValueError(f"Unknown prompt type {prompt_type}")
                return (
                    self.prompt_templates[prompt_type](chat_style=chat_style, chat_history=chat_history),
                    dict(
                        qn_subject=qn_subject,
                        prompt_prefix=prompt_prefix,
                        context_prefix=context_prefix,
                    ),
                    answers,
                )

        return ret

    def generate_context(self, statement_content, prompt_id, template_context):
        '''
        Returns:
            cur_ctx, ctx_idx_map
            ctx_idx_map: {
                'name': List[List[Substring]],
                'country': List[List[Substring]],
                'food': List[List[Substring]],
            }
        note: ctx_idx_map is a list of lists because things can occur multiple times in fixed templates
        '''
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_context.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            assert context_type in ["random", "basic"] or isinstance(context_type, int)
            if context_type == "random":
                raise NotImplementedError("Random context not implemented yet")
            elif context_type == "basic":
                context_template = self.basic_context_template
            else:
                context_template = self.context_templates[context_type]
            with set_prompt_id(prompt_id, self.names, self.capitals, self.foods):
                fields = statement_content[: len(statement_content) - 1]
                statement_type = statement_content.type or self.default_statement_type

                cur_ctx, ctx_idx_map = TrackFormatter().format(
                    context_template,
                    **getattr(self.statement_formatters, statement_type)(self, *fields),
                )

                return (
                    cur_ctx,
                    {
                        "name": [
                            ctx_idx_map[f"name_{i}"]
                            for i in range(len(statement_content.attr0))
                        ],
                        "country": [
                            ctx_idx_map[f"country_{i}"]
                            for i in range(len(statement_content.attr1))
                        ],
                        "food": [
                            ctx_idx_map[f"food_{i}"]
                            for i in range(len(statement_content.attr2))
                        ],
                        "sentence": Substring(0, len(cur_ctx)),
                    },
                )
        return ret
    def get_standard_context(self, num_entities):
        x = [i for i in range(num_entities)]
        return [tc.TStatement(x, x, x, None)]
    
    def get_predicates(self, context_content, prompt_id):
        assert len(context_content) == 1
        true_predicates = []
        with set_prompt_id(prompt_id, self.names, self.capitals, self.foods):
            for name, country, food in zip(context_content[0].attr0, context_content[0].attr1, context_content[0].attr2):
                true_predicates.append((
                    self.names[name],
                    self.foods[food],
                    self.foods.type
                ))
                true_predicates.append((
                    self.names[name],
                    self.capitals[country],
                    self.capitals.type
                ))
        return true_predicates
    def canonize_token_maps(self, stacked_token_maps):
        """
        Extracts the location of the first token of all 'attr' and 'name'.
        Deals with parallel structure.

        Output:
        {
            'name': List[IntTensor[batch]]
            'attr': List[IntTensor[batch]]
        }
        """
        raise NotImplementedError("Not implemented yet")



class NameCountryVaryFixedTemplate:
    prompt_templates =  {
        'default': lambda chat_style, chat_history: (
            "{prompt_prefix}" + 
            chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} lives in the country of"])
        )
    }
    context_templates = {
        'series': "{name_0} lives in {country_0}. {name_1} lives in {country_1}.",
        'cross': "{name_0} and {name_1} live in {country_0} and {country_1} respectively.",
        'nested_2': "{name_0}, unlike {name_1} who lives in {country_1}, lives in {country_0}.",
        'coref': "{name_0} and {name_1} are friends. The former lives in {country_0}. The latter lives in {country_1}.",
        'nested': "{name_0} and {name_1} are friends. The latter lives in {country_1}. The former lives in {country_0}.",
        'medium': "{name_0}, a {noun_phrase_0}, lives in {country_0}. {name_1} lives in {country_1}.",
        'long': "{name_0}, a {noun_phrase_0} who {verb_phrase_0}, lives in {country_0}. {name_1} lives in {country_1}.",
        'reverse': "{name_0} lives in {country_0}. {country_1} is where {name_1} lives.",
    }
    class statement_formatters:
        def parallel(self, names, countries):
            return dict(
                [(f"name_{i}", self.names[name]) for i, name in enumerate(names)]
                + [
                    (f"country_{i}", self.capitals[attr][0])
                    for i, attr in enumerate(countries)
                ]
            )

    def __init__(self, tokenizer_type, split=None):
        self.default_template_content = dict(
            prompt_type = 'default',
            chat_style="llama_chat",
            context_type = 'series', # "series", "cross", "nested", "medium", "long", "nested_2", "reverse", "coref"
            prompt_prefix = "",
            context_prefix = "",
            chat_history = [],
        )
        self.default_statement_type = "parallel"
        self.names = NameDomain(tokenizer_type=tokenizer_type, split=split)
        self.capitals = CapitalDomain(tokenizer_type=tokenizer_type, split=split)
        self.noun_phrases = NounPhraseDomain(tokenizer_type=tokenizer_type, split=split)
        self.verb_phrases = VerbPhraseDomain(tokenizer_type=tokenizer_type, split=split)
        self.tokenizer = self.names.tokenizer

    def extract_template_indices(self, full_output_indices):
        return {
            "qn_subject": full_output_indices["qn_subject"][0],
            "ans_subject": full_output_indices["qn_subject"][-1],
        }

    def generate_template(
        self, prompt_id, template_content, context_content, num_answers
    ):
        """
        Returns:
            (prompt : str, template_substitutions : Dict, answer: List[int])
        """
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_content.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            with set_prompt_id(prompt_id, self.names, self.capitals):
                qn_subject = self.names[query_name]
                answers = [
                    self.capitals.encode_single_word(self.capitals[i][0])
                    for i in range(num_answers)
                ]
                return (
                    self.prompt_templates[prompt_type](chat_style=chat_style, chat_history=chat_history),
                    dict(
                        qn_subject=qn_subject,
                        prompt_prefix=prompt_prefix,
                        context_prefix=context_prefix,
                    ),
                    answers,
                )

        return ret

    def generate_context(self, statement_content, prompt_id, template_context):
        '''
        Returns:
            cur_ctx, ctx_idx_map
            ctx_idx_map: {
                'name': List[List[Substring]],
                'country': List[List[Substring]],
            }
        note: ctx_idx_map is a list of lists because things can occur multiple times in fixed templates
        '''
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_context.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            context_template = self.context_templates[context_type]
            with set_prompt_id(prompt_id, self.names, self.capitals, self.noun_phrases, self.verb_phrases):
                fields = statement_content[: len(statement_content) - 1]
                statement_type = statement_content.type or self.default_statement_type
                
                noun_phrases = {}
                if context_type == 'medium' or context_type == 'long':
                    noun_phrases = {
                        'noun_phrase_0': self.noun_phrases[0],
                        # 'noun_phrase_1': self.noun_phrases[1],
                    }
                verb_phrases = {}
                if context_type == 'long':
                    verb_phrases = {
                        'verb_phrase_0': self.verb_phrases[0],
                        # 'verb_phrase_1': self.verb_phrases[1],
                    }
                cur_ctx, ctx_idx_map = TrackFormatter().format(
                    context_template,
                    **getattr(self.statement_formatters, statement_type)(self, *fields),
                    **noun_phrases,
                    **verb_phrases
                )

                return (
                    cur_ctx,
                    {
                        "name": [
                            ctx_idx_map[f"name_{i}"]
                            for i in range(len(statement_content.name))
                        ],
                        "country": [
                            ctx_idx_map[f"country_{i}"]
                            for i in range(len(statement_content.attr))
                        ],
                        "sentence": Substring(0, len(cur_ctx)),
                    },
                )
        return ret
    def get_standard_context(self, num_entities):
        x = [i for i in range(num_entities)]
        return [tc.Statement(x, x, None)]
    
    def get_predicates(self, context_content, prompt_id):
        assert len(context_content) == 1
        true_predicates = []
        with set_prompt_id(prompt_id, self.names, self.capitals):
            for name, country in zip(context_content[0].name, context_content[0].attr):
                true_predicates.append((
                    self.names[name],
                    self.capitals[country],
                    self.capitals.type
                ))
        return true_predicates
    def canonize_token_maps(self, stacked_token_maps):
        """
        Extracts the location of the first token of all 'attr' and 'name'.
        Deals with parallel structure.

        Output:
        {
            'name': List[IntTensor[batch]]
            'attr': List[IntTensor[batch]]
        }
        """
        raise NotImplementedError("Not implemented yet")


class NameCountryOccupationFixedTemplate:
    class statement_formatters:
        def parallel(self, names, countries, occupations):
            return dict(
                [(f"name_{i}", self.names[name]) for i, name in enumerate(names)]
                + [
                    (f"country_{i}", self.capitals[attr][0])
                    for i, attr in enumerate(countries)
                ]+  [
                    (f"occupation_{i}", self.occupations[attr])
                    for i, attr in enumerate(occupations)
                ]
            )
    default_statement_type = "parallel"
    prompt_templates = {
        "chat_name_country": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} lives in the country of"]),
        "chat_name_occupation": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} has the occupation of"]),
    }
    basic_context_template = (
        "{name_0} lives in {country_0}. {name_1} lives in {country_1}. "
        "{name_0} works as {occupation_0}. {name_1} works as {occupation_1}."
    )

    def __init__(self, tokenizer_type, split=None):
        self.default_template_content = dict(
            prompt_type = "chat_name_country",
            chat_style="llama_chat",
            context_type = 'basic', # "basic", "random", or int
            prompt_prefix = "",
            context_prefix = "",
            chat_history = [],
        )
        self.names = NameDomain(tokenizer_type=tokenizer_type, split=split)
        self.capitals = CapitalDomain(tokenizer_type=tokenizer_type, split=split)
        self.occupations = OccupationDomain(tokenizer_type=tokenizer_type, split=split)
        self.tokenizer = self.names.tokenizer

        # with open(os.path.join(COREF_ROOT, "coref/datasets/auto_gen/gen_templates/name_country_food_2.yaml")) as f:
        #     self.context_templates = yaml.safe_load(f)
        self.context_templates = {} # TODO: fill with real templates

    def extract_template_indices(self, full_output_indices):
        return {
            "qn_subject": full_output_indices["qn_subject"][0],
            "ans_subject": full_output_indices["qn_subject"][-1],
        }

    def generate_template(
        self, prompt_id, template_content, context_content, num_answers
    ):
        """
        Returns:
            (prompt : str, template_substitutions : Dict, answer: List[int])
        """
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_content.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            with set_prompt_id(prompt_id, self.names, self.capitals, self.occupations):
                if prompt_type == "chat_name_country":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.capitals.encode_single_word(self.capitals[i][0])
                        for i in range(num_answers)
                    ]
                elif prompt_type == "chat_name_occupation":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.occupations.encode_single_word(self.occupations[i])
                        for i in range(num_answers)
                    ]
                else:
                    raise ValueError(f"Unknown prompt type {prompt_type}")
                return (
                    self.prompt_templates[prompt_type](chat_style=chat_style, chat_history=chat_history),
                    dict(
                        qn_subject=qn_subject,
                        prompt_prefix=prompt_prefix,
                        context_prefix=context_prefix,
                    ),
                    answers,
                )

        return ret

    def generate_context(self, statement_content, prompt_id, template_context):
        '''
        Returns:
            cur_ctx, ctx_idx_map
            ctx_idx_map: {
                'name': List[List[Substring]],
                'country': List[List[Substring]],
                'occupation': List[List[Substring]],
            }
        note: ctx_idx_map is a list of lists because things can occur multiple times in fixed templates
        '''
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_context.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            assert context_type in ["random", "basic"] or isinstance(context_type, int)
            if context_type == "random":
                raise NotImplementedError("Random context not implemented yet")
            elif context_type == "basic":
                context_template = self.basic_context_template
            else:
                context_template = self.context_templates[context_type]
            with set_prompt_id(prompt_id, self.names, self.capitals, self.occupations):
                fields = statement_content.fields
                statement_type = statement_content.type or self.default_statement_type

                cur_ctx, ctx_idx_map = TrackFormatter().format(
                    context_template,
                    **getattr(self.statement_formatters, statement_type)(self, *fields),
                )

                return (
                    cur_ctx,
                    {
                        "name": [
                            ctx_idx_map[f"name_{i}"]
                            for i in range(len(statement_content.fields[0]))
                        ],
                        "country": [
                            ctx_idx_map[f"country_{i}"]
                            for i in range(len(statement_content.fields[1]))
                        ],
                        "occupation": [
                            ctx_idx_map[f"occupation_{i}"]
                            for i in range(len(statement_content.fields[2]))
                        ],
                        "sentence": Substring(0, len(cur_ctx)),
                    },
                )
        return ret
    def get_standard_context(self, num_entities):
        x = [i for i in range(num_entities)]
        return [tc.VarStatement([x,x,x], None)]
    
    def get_predicates(self, context_content, prompt_id):
        assert len(context_content) == 1
        true_predicates = []
        with set_prompt_id(prompt_id, self.names, self.capitals, self.occupations):
            for name, country, occupation in zip(*context_content[0].fields):
                true_predicates.append((
                    self.names[name],
                    self.capitals[country],
                    self.capitals.type
                ))
                true_predicates.append((
                    self.names[name],
                    self.occupations[occupation],
                    self.occupations.type
                ))
        return true_predicates
    def canonize_token_maps(self, stacked_token_maps):
        """
        Extracts the location of the first token of all 'attr' and 'name'.
        Deals with parallel structure.

        Output:
        {
            'name': List[IntTensor[batch]]
            'attr': List[IntTensor[batch]]
        }
        """
        raise NotImplementedError("Not implemented yet")

class NameCountryFoodOccupationFixedTemplate:
    class statement_formatters:
        def parallel(self, names, countries, foods, occupations):
            return dict(
                [(f"name_{i}", self.names[name]) for i, name in enumerate(names)]
                + [
                    (f"country_{i}", self.capitals[attr][0])
                    for i, attr in enumerate(countries)
                ]+ [
                    (f"food_{i}", self.foods[attr])
                    for i, attr in enumerate(foods)
                ]+ [
                    (f"occupation_{i}", self.occupations[attr])
                    for i, attr in enumerate(occupations)
                ]
            )
    default_statement_type = "parallel"
    prompt_templates = {
        "chat_name_country": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} lives in the country of"]),
        "chat_name_food": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} likes to eat"]),
        "chat_name_occupation": lambda chat_style, chat_history: "{prompt_prefix}" + chat_formatter(chat_style, chat_history+["{context_prefix}{context}", "Therefore, {qn_subject} has the occupation of"]),
    }
    basic_context_template = (
        "{name_0} lives in {country_0}. {name_1} lives in {country_1}. "
        "{name_0} likes to eat {food_0}. {name_1} likes to eat {food_1}. "
        "{name_0} works as {occupation_0}. {name_1} works as {occupation_1}."
    )

    def __init__(self, tokenizer_type, split=None):
        self.default_template_content = dict(
            prompt_type = "chat_name_country",
            chat_style="llama_chat",
            context_type = 'basic', # "basic", "random", or int
            prompt_prefix = "",
            context_prefix = "",
            chat_history = [],
        )
        self.names = NameDomain(tokenizer_type=tokenizer_type, split=split)
        self.capitals = CapitalDomain(tokenizer_type=tokenizer_type, split=split)
        self.foods = FoodDomain(tokenizer_type=tokenizer_type, split=split)
        self.occupations = OccupationDomain(tokenizer_type=tokenizer_type, split=split)
        self.tokenizer = self.names.tokenizer

        # with open(os.path.join(COREF_ROOT, "coref/datasets/auto_gen/gen_templates/name_country_food_2.yaml")) as f:
        #     self.context_templates = yaml.safe_load(f)
        self.context_templates = {} # TODO: fill with real templates

    def extract_template_indices(self, full_output_indices):
        return {
            "qn_subject": full_output_indices["qn_subject"][0],
            "ans_subject": full_output_indices["qn_subject"][-1],
        }

    def generate_template(
        self, prompt_id, template_content, context_content, num_answers
    ):
        """
        Returns:
            (prompt : str, template_substitutions : Dict, answer: List[int])
        """
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_content.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            with set_prompt_id(prompt_id, self.names, self.capitals, self.foods, self.occupations):
                if prompt_type == "chat_name_country":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.capitals.encode_single_word(self.capitals[i][0])
                        for i in range(num_answers)
                    ]
                elif prompt_type == "chat_name_food":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.foods.encode_single_word(self.foods[i], suppress_error=True)
                        for i in range(num_answers)
                    ]
                elif prompt_type == "chat_name_occupation":
                    qn_subject = self.names[query_name]
                    answers = [
                        self.occupations.encode_single_word(self.occupations[i])
                        for i in range(num_answers)
                    ]
                else:
                    raise ValueError(f"Unknown prompt type {prompt_type}")
                return (
                    self.prompt_templates[prompt_type](chat_style=chat_style, chat_history=chat_history),
                    dict(
                        qn_subject=qn_subject,
                        prompt_prefix=prompt_prefix,
                        context_prefix=context_prefix,
                    ),
                    answers,
                )

        return ret

    def generate_context(self, statement_content, prompt_id, template_context):
        '''
        Returns:
            cur_ctx, ctx_idx_map
            ctx_idx_map: {
                'name': List[List[Substring]],
                'country': List[List[Substring]],
                'food': List[List[Substring]],
            }
        note: ctx_idx_map is a list of lists because things can occur multiple times in fixed templates
        '''
        new_template_content = {**self.default_template_content, **{
            k: v
            for k, v in template_context.items()
            if v is not None
        }} # None values forces a fallback to default
        @lambda _: _(**new_template_content)
        def ret(query_name, prompt_type, prompt_prefix, chat_history, context_prefix, chat_style, context_type):
            assert context_type in ["random", "basic"] or isinstance(context_type, int)
            if context_type == "random":
                raise NotImplementedError("Random context not implemented yet")
            elif context_type == "basic":
                context_template = self.basic_context_template
            else:
                context_template = self.context_templates[context_type]
            with set_prompt_id(prompt_id, self.names, self.capitals, self.foods, self.occupations):
                fields = statement_content.fields
                statement_type = statement_content.type or self.default_statement_type

                cur_ctx, ctx_idx_map = TrackFormatter().format(
                    context_template,
                    **getattr(self.statement_formatters, statement_type)(self, *fields),
                )

                return (
                    cur_ctx,
                    {
                        "name": [
                            ctx_idx_map[f"name_{i}"]
                            for i in range(len(statement_content.fields[0]))
                        ],
                        "country": [
                            ctx_idx_map[f"country_{i}"]
                            for i in range(len(statement_content.fields[1]))
                        ],
                        "food": [
                            ctx_idx_map[f"food_{i}"]
                            for i in range(len(statement_content.fields[2]))
                        ],
                        "occupation": [
                            ctx_idx_map[f"occupation_{i}"]
                            for i in range(len(statement_content.fields[3]))
                        ],
                        "sentence": Substring(0, len(cur_ctx)),
                    },
                )
        return ret
    def get_standard_context(self, num_entities):
        x = [i for i in range(num_entities)]
        return [tc.VarStatement([x,x,x,x], None)]
    
    def get_predicates(self, context_content, prompt_id):
        assert len(context_content) == 1
        true_predicates = []
        with set_prompt_id(prompt_id, self.names, self.capitals, self.foods, self.occupations):
            for name, country, food, occupation in zip(*context_content[0].fields):
                true_predicates.append((
                    self.names[name],
                    self.foods[food],
                    self.foods.type
                ))
                true_predicates.append((
                    self.names[name],
                    self.capitals[country],
                    self.capitals.type
                ))
                true_predicates.append((
                    self.names[name],
                    self.occupations[occupation],
                    self.occupations.type
                ))
        return true_predicates
    def canonize_token_maps(self, stacked_token_maps):
        """
        Extracts the location of the first token of all 'attr' and 'name'.
        Deals with parallel structure.

        Output:
        {
            'name': List[IntTensor[batch]]
            'attr': List[IntTensor[batch]]
        }
        """
        raise NotImplementedError("Not implemented yet")